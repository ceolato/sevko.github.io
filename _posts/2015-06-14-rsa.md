---
title: "RSA: implementation and proofs"
description: A Python implementation of RSA, and formal proofs of its sub-algorithms.
code: true
math: true
date: 2015-06-14 21:00:00
thumbnail:
  link: prime_number_spiral.png
  alt: An Ulam prime-number spiral.
tags:
  - math
  - prime numbers
  - algorithms
  - python
---

# outline
foo

1. what is RSA?
  - what is it?
  - what's the history?
  - what is it used for?
2. the goals of this article
  - to implement RSA, and thoroughly understand each of its moving parts
  - we will delve into all of the algorithms a typical RSA implementation uses, and prove them
  - we WILL NOT currently prove RSA itself, though this might be added as an extension to the post in the future
3. a note about the math
  - the proofs don't require much formal math background
  - modular arithmetic
    - a gotcha about (mod n) notation
  - define coprime
4. generate a key-pair
  - the steps
  - the problems introduced by each step
  - calculate phi(n)
  - efficiently calculate GCD: euclidean algorithm
  - find modular inverses
  - test for primality: Rabin Miller
5. encryption/decryption
 - the steps
 - modular exponentiation

# what is RSA?
[RSA](https://en.wikipedia.org/wiki/RSA_(cryptosystem)) is a *public-key*, or *asymmetric*, encryption algorithm.
Unlike *symmetric* algorithms, like [DES](https://en.wikipedia.org/wiki/Data_Encryption_Standard) and
[AES](https://en.wikipedia.org/wiki/Advanced_Encryption_Standard), which use the same key for encrypting and decrypting
data, RSA uses two distinct keys: a **public** key (published for others to see) used to encrypt data, and a **private
key** (kept secret) used to decrypt whatever was encrypted with the public one. The beauty of public-key encryption is
that the parties involved never need to exchange and protect a master key, which is what symmetric algorithms require:
thus, communications can be encrypted without any kind of exchange beforehand.

Public-key encryption was proposed by [Whitfield Diffie](https://en.wikipedia.org/wiki/Whitfield_Diffie) and [Martin
Hellman](https://en.wikipedia.org/wiki/Martin_Hellman) in '76, while RSA itself was patented in '77 by [Ron
**R**ivest](https://en.wikipedia.org/wiki/Ron_Rivest), [Adi **S**hamir](https://en.wikipedia.org/wiki/Adi_Shamir), and
[Leonard **A**dleman](https://en.wikipedia.org/wiki/Leonard_Adleman).

![Rivest, Shamir, and Adleman](/img/rivest_shamir_adleman.png)

[Clifford Cocks](https://en.wikipedia.org/wiki/Clifford_Cocks), an English cryptographer, arrived at a similar
algorithm in '73 while working for British intelligence at
[GHCQ](https://en.wikipedia.org/wiki/Government_Communications_Headquarters), but his work wasn't declassified until
1998.

# the goals of this article
I struggled to understand RSA for a long time: not even the encryption itself, but its implementation. RSA employs a
number of other elegant and powerful algorithms, like the Euclidean algorithm and probabilistic primality tests, each
of which struck me as worth learning in its own right. So, I felt a little uncomfortable to "just use the Extended
Euclidean algorithm to find the modular inverse of a value" without knowing how the Extended Euclidean algorithm
worked, which itself required knowing how the vanilla Euclidean algorithm works, and so on. This post will implement
RSA, but also thoroughly explain and formally prove *each* of the other algorithms that it depends on. Note that I will
**not** prove RSA itself, because it's more involved than all of the other proofs and requires more background going
in (like some basic [group theory](https://en.wikipedia.org/?title=Group_theory)); I might add it as an extension to
this post in the future.

# math precursor
**TODO**: this needs to be written later, once I actually figure out what you need to know going in.
The only thing we need to know before diving into RSA is some [modular
arithmetic](https://en.wikipedia.org/wiki/Modular_arithmetic), which is simply arithmetic with the property that
numbers have a maximum value (called the *modulus*) and wrap around to 0 when they exceed it.

$$a \equiv b \pmod c$$

Here, the $$\equiv$$ symbol implies *congruence*, or that $$a \text{ mod } c$$ equals $$b \text{ mod } c$$.

# how RSA works
RSA revolves around a numeric key-pair, or a mathematically related public and private key. The public key is made
known to the world, which can then use it to encrypt a message, while the private key can be used to decrypt anything
encrypted with the public key. Encrypting and decrypting a message is fairly straightforward, while generating a
key-pair is a more substantial process.

# generate a key-pair
To generate a public/private key-pair:

  1. generate two (large) primes, $$p$$ and $$q$$
  2. let $$n = p \times q$$
  3. find $$\phi(n)$$ ([Euler's totient](https://en.wikipedia.org/wiki/Euler's_totient_function)), or the number of
     integers in the range $$[1, n]$$ that are coprime with $$n$$ -- that is, have a Greatest Common Divisor (GCD) of 1
     with it.
  4. find a value $$e$$ such that $$1 \lt e \lt \phi(n)$$ and $$e$$ is coprime with $$\phi(n)$$; this is your **public
     key**.
  5. find a value $$d$$ such that $$d \times e \equiv 1 \pmod{\phi(n)}$$, or the [multiplicative modular
     inverse](https://en.wikipedia.org/wiki/Modular_multiplicative_inverse) of $$e$$ modulo $$\phi(n)$$; this is your
     **private key**.

Though short and concise, the above steps present several complex problems:

  1. generate a large, random prime number (**step 1**)
  2. find $$\phi(n)$$, where $$n$$ is the product of two primes (**step 3**)
  3. find the GCD of two numbers, which will allow us to compute $$e$$ (**step 4**)
  4. find the multiplicative modular inverse of a value, to find $$d$$ (**step 4**)

## finding $$\phi(n)$$
To compute $$\phi(n)$$, we can take advantage of the fact that it's composed of two **prime** factors: $$p$$ and $$q$$.
Thus, the only values with which it shares GCDs that aren't 1 must be multiples of either $$p$$ or $$q$$ (for instance,
$$\gcd(n, 2q) = q$$ and $$\gcd(n, 3p) = p$$). There are only $$q$$ multiples of $$p$$ ($$p, 2p, 3p, \ldots, qp$$) and
$$p$$ multiples of $$q$$ ($$q, 2q, 3q, \ldots, qp$$) that are less than or equal to $$n$$. Thus, there are $$q + p$$
values in the range $$[1, n]$$ that have a GCD with $$n$$ not equal to 1. Note, however, that we double counted $$pq$$
in our list of multiples of $$p$$ and $$q$$, so in reality it's $$p + q - 1$$. Thus, $$\phi(n) = \text{total} - (p + q
-1)$$, where $$\text{total}$$ is the total numbers of values in the range $$[1, n]$$ -- which is $$n$$.

$$\phi(n) = n - (p + q - 1) = n - p - q + 1$$

## computing GCDs
To find the GCD of two numbers, we'll employ the [Euclidean
algorithm](https://en.wikipedia.org/?title=Euclidean_algorithm):

  1. the GCD of any number and 0 is the absolute value of that number
  2. the GCD of numbers $$a$$ and $$b$$ is the GCD of $$b$$ and $$a \text{ mod } b$$

or:

{% highlight python table %}
def gcd(a, b):
    return a if b == 0 else gcd(b, a % b)
{% endhighlight %}

$$
  \def \imod {\text{ mod }}
\def \divs {\text{ } \vert \text{ }}
$$
Let's prove it. Case 1 should be self-explanatory: 0 is technically divisible by any number, even if the quotient
equals 0, so the GCD of 0 and any other number should be that number. We need to be careful and take its absolute
value, however, to account for negative values; the greatest divisor of -5 is 5, after all, not -5, so the
GCD of 0 and -5 must also be 5. Thus, we have to take the absolute value of -5 to arrive at the greatest divisor.

Case 2 is less intuitive (at least for me), and requires proving that $$\gcd(a, b) = \gcd(b, a \imod b)$$. Let's begin
by creating another variable $$c$$:

$$
c = a - b
$$

### prove $$\gcd(a, b) \divs c$$
We first want to prove that the GCD of $$a$$ and $$b$$ divides $$c$$ (or $$\gcd(a, b) \divs c$$).

$$
a = x \cdot \gcd(a, b)\\
b = y \cdot \gcd(a, b)\\
c = a - b\\
c = x \cdot \gcd(a, b) - y \cdot \gcd(a, b) = (x - y) \gcd(a, b)\\
\therefore \gcd(a, b) \divs c
$$

### prove $$\gcd(b, c) \divs a$$

$$
b = x \cdot \gcd(b, c)\\
c = y \cdot \gcd(b, c)\\
a = c + b\\
a = x \cdot \gcd(b, c) + y \cdot \gcd(b, c) = (x + y) \gcd(b, c)\\
\therefore \gcd(b, c) \divs a
$$

### prove $$\gcd(a, b) = \gcd(b, a - b)$$
We know that, by definition, $$\gcd(a, b) \divs b$$, and we've proven that $$\gcd(a, b) \divs c$$. Thus, $$\gcd(a, b)$$
is a *common divisor* of both $$b$$ and $$c$$. That doesn't imply that it's the least common divisor, greatest, or
anything else: all we know is that it divides both numbers. We *do* know that there exists a **greatest** common
divisor of $$b$$ and $$c$$, $$\gcd(b, c)$$, so we can conclude that:

$$\gcd(a, b) \le \gcd(b, c)$$

We now re-apply that same reasoning. We know that $$\gcd(b, c) \divs b$$ and $$\gcd(b, c) \divs a$$. Thus, $$\gcd(b,
c)$$ is a common divisor of $$b$$ and $$a$$. Since we know that the **greatest** common divisor of $$a$$ and $$b$$ is
$$\gcd(a, b)$$, we can conclude that:

$$\gcd(b, c) \le \gcd(a, b)$$

But now we have two almost contradictory conclusions:

$$
\gcd(a, b) \le \gcd(b, c)\\
\gcd(b, c) \le \gcd(a, b)
$$

The only way these can both be true is if:

$$\gcd(a, b) = \gcd(b, c)$$

So we've proven that $$\gcd(a, b) = \gcd(b, a - b)$$ (remember, $$c = a - b$$).

### prove $$\gcd(b, a - b) = \gcd(b, a \imod b)$$
First, let's assume that $$a > b$$, and rewrite it as: $$a = bq + r$$ (or $$r = a \imod b$$)

Now, we already know that $$\gcd(a, b) = \gcd(b, a - b)$$, Since order doesn't matter, we can rewrite $$\gcd(b, a -
b)$$ as $$\gcd(a - b, b)$$. Now, we apply the rule $$\gcd(a, b) = \gcd(b, a - b)$$ again.

$$
\gcd(a, b) = \gcd(b, a - b) = \gcd(a - b, b)\\
\gcd(a - b, b) = \gcd(b, a - b - b) = \gcd(a - 2b, b)\\
\gcd(a - 2b, b) = \gcd(b, a - 2b - b) = \gcd(a - 3b, b)\\
\ldots\\
\gcd(a - qb, b) = \gcd(r, b)
$$

or:

$$\gcd(a, b) = \gcd(a - b, b) = \gcd(a - 2b, b) = \ldots = \gcd(a - qb, b) = \gcd(r, b)$$

Bingo. We've proven Case 2.

## finding modular inverses
Given a value $$a$$ and modulus $$c$$, the modular multiplicative inverse of $$a$$ is a value $$b$$ that satisfies:

$$ab \equiv 1 \pmod c$$

This implies that there exists some value $$d$$ for which:

$$
ab = 1 + cd\\
ab - cd = 1
$$

This turns out to be in the form of [Bézout's identity](https://en.wikipedia.org/wiki/B%C3%A9zout's_identity), which
states that for values $$a$$ and $$b$$, there exist values $$x$$ and $$y$$ that satisfy:

$$
ax + by = \gcd(a, b)
$$

$$x$$ and $$y$$, called Bézout coefficients, can be solved for using the [Extended Euclidean
algorithm](https://en.wikipedia.org/wiki/Extended_Euclidean_algorithm) (EEA). $$x$$ corresponds to $$b$$, or the
modular inverse that we were looking for, while $$y$$ can be thrown out once computed. The EEA will also give us the
GCD of $$a$$ and $$b$$ -- it is, after all, an extension of the Euclidean algorithm, which we use to find the GCD of
two values. We need to verify that it equals 1, since we make the assume that $$\gcd(a, b) = 1$$; if it doesn't, $$a$$
has no modular inverse. Since `modular_inverse()` is just a wrapper for EEA -- to be implemented in a function called
`bezout_coefficients()` -- its definition is simple:

{% highlight python table %}
def modular_inverse(num, modulus):
    coef1, _, gcd = bezout_coefficients(num, modulus)
    return coef1 if gcd == 1 else None
{% endhighlight %}

`bezout_coefficients()` is a bit tricker:

{% highlight python table %}
def bezout_coefficients(a, b):
    if b == 0:
        return -1 if a < 0 else 1, 0, abs(a)
    else:
        quotient, remainder = divmod(a, b)
        coef1, coef2, gcd = bezout_coefficients(b, remainder)
        return coef2, coef1 - quotient * coef2, gcd
{% endhighlight %}

Let's see why it works.

### the Extended Euclidean algorithm
How to solve for $$x$$ and $$y$$? Bezout's Identity states:

$$
\gcd(a, b) = ax + by\\
$$

or, for $$\gcd(b, a \imod b)$$:

$$
\gcd(b, a \imod b) = bx' + (a \imod b)y'\\
$$

Let's simplify:

$$
a \imod b = a - \lfloor \frac{a}{b} \rfloor b\\
\gcd(b, a \imod b) = bx' + (a - \lfloor \frac{a}{b} \rfloor b)y' = bx' + ay' - \lfloor \frac{a}{b} \rfloor by' =\\
   ay' + b(x' - \lfloor \frac{a}{b} \rfloor y')
$$

Since we know, by the already proven Euclidean algorithm, that $$\gcd(a, b) = \gcd(b, a \imod b)$$, we can write:

$$
ax + by = ay' + b(x' - \lfloor \frac{a}{b} \rfloor y')
$$

So, $$x = y'$$ and $$y = x' - \lfloor \frac{a}{b} \rfloor y'$$. But what are $$x'$$ and $$y'$$? They're the results of
running the EEA on $$(b, a \imod b)$$! Classic recursion. In sum:

{% highlight python table %}
def bezout_coefficients(a, b):
    quotient, remainder = divmod(a, b)
    coef1, coef2 = bezout_coefficients(b, remainder)
    return coef2, coef1 - quotient * coef2
{% endhighlight %}

Of course, we need a base case, or we'll end up recursing *ad infinitum*. Let's take the case of $$b = 0$$.

$$
ax + by = \gcd(a, b)\\
b = 0\\
ax + 0y = \gcd(a, 0)\\
ax = |a|\\
x = \frac{|a|}{a}
$$

So, if $$b = 0$$, we set the $$x$$ coefficient to the 1 if $$a$$ is positive and -1 is $$a$$ is negative, and set $$y$$
to... what? If $$b$$ is 0, then $$y$$ can take on any value. For simplicity's sake we'll choose 0. Our revised
definition looks like:

{% highlight python table %}
def bezout_coefficients(a, b):
    if b == 0:
        return -1 if a < 0 else 1, 0
    else:
        quotient, remainder = divmod(a, b)
        coef1, coef2 = bezout_coefficients(b, remainder)
        return coef2, coef1 - quotient * coef2
{% endhighlight %}

Also note that, since this is simply a more involved version of the Euclidean algorithm (we're making recursive calls
to `bezout_coefficients(b, remainder)` and have a base case of `b == 0`), when we hit the base case, `abs(a)` is the
GCD of `a` and `b`. Since `modular_inverse()` needs to check that the GCD of its two arguments equals 1, we should
return it as well. Hence:

{% highlight python table %}
def bezout_coefficients(a, b):
    if b == 0:
        return -1 if a < 0 else 1, 0, abs(a)
    else:
        quotient, remainder = divmod(a, b)
        coef1, coef2, gcd = bezout_coefficients(b, remainder)
        return coef2, coef1 - quotient * coef2, gcd
{% endhighlight %}

## generating large, random primes
Here's the idea:

  1. generate a large, random, **odd** number $$x$$
  2. check $$x$$ for primality
     1. if $$x$$ prime, return it
     2. otherwise, increment $$x$$ by 2, and return to step **2.)**

Easy enough, except for the bit about testing primality. How do we efficiently do so? We'll turn to the
[Rabin-Miller](https://en.wikipedia.org/wiki/Miller%E2%80%93Rabin_primality_test) algorithm, a probabilistic primality
test which either tells us with absolute certainty that a number is composite, or with high likelihood that it's prime.
We're fine with a merely probabilistic solution because it's *fast*, since speed is a non-negligible issue due to the
size of the numbers that we're dealing with, and also because the chances of a false positive (ie indicating that a
number is prime when it's actually composite) are astronomically low after even only a few iterations of the test.

### Rabin-Miller primality test
The Rabin-Miller test relies on the below two assumptions (just assume they're true for now, and we'll prove them later
on). If $$p$$ is a prime number:

  1. $$a ^ {p - 1} \equiv 1 \pmod p$$ for any $$a$$
  2. for any $$x$$ that satisfies $$x ^ 2 \equiv 1 \pmod p$$, $$x$$ **must** equal ±1

Using these, you can test a value $$n$$ for compositeness like so (note that we return `true`/`false` to indicate
definite compositeness/probable primality respectively):

  1. pick a random value $$a$$ in the range $$[2, n - 1]$$
  2. use **assumption 1** to assert that $$a ^ {n - 1} \equiv 1 \pmod n$$); if it's not, return `true`
  3. if $$a$$ has an integer square root, let $$a' = \sqrt a$$; otherwise, return `false`
  4. since $$a' ^ 2 \equiv 1 \pmod n$$, we can use **assumption 2** to assert that $$a' \equiv \pm 1 \pmod n$$; if not,
     return `true`
  5. otherwise, repeat steps 3-4, taking the square root of $$a'$$, and the square root of that, and so on, until you
     hit a value that doesn't have an integer square root.
  6. if you haven't already returned anything, you've satisfied assumptions **1** and **2** for all testable cases and
     can return `false`.

In result, we return `true` if we've confirmed that $$a$$ is a *witness to the compositeness* of $$n$$, and `false` if
$$a$$ does *not* prove that $$n$$ is composite -- transitively, there is a high chance that $$n$$ is prime, but we can
only be more sure by running more such tests. While the above steps serve as a good verbal description of the
algorithm, we'll have to slightly modify them to convert the algorithm into real code.

We need to implement a function `is_witness()`, which checks whether a random value is a witness to the compositeness
of our prime candidate, $$n$$.

  1. write $$n - 1$$ in the form $$2 ^ s d$$. $$n=73$$, for instance, would yield $$s=3$$ and $$d=9$$, since $$73 - 1 =
     72 = 2 ^ 3 \cdot 9$$.
  2. pick a random value $$a$$ in the range $$[2, n - 1]$$. We'll check whether this is a witness for $$n$$.
  3. let $$x = a ^ d \imod n$$
  4. if $$x \equiv \pm 1 \pmod n$$, then return `false`
  5. repeat $$s - 1$$ times:
     1. let $$x = x ^ 2 \imod n$$
     2. if $$x = 1$$, return `true`
     3. if $$x = n - 1$$, return `false`
  6. if we haven't returned yet, return `true`

These steps seem quite a bit different from before, but in reality, they're exactly the same and just operating in
reverse. We start with a value that doesn't have an integer square root, and square it until we hit $$a ^ {n - 1}$$.
Why did we bother decomposing $$n - 1$$ into the form of $$2 ^ s d$$? Well, it allows us to rewrite $$a ^ {n - 1}$$
as $$a ^ {2 ^ s d}$$, and now we know **exactly** how many times we can take square roots before we hit a value that
isn't reducible any further -- in this case, $$a ^ d$$.

$$
a_1 = \sqrt{a ^ {2 ^ s d}} = (a ^ {2 ^ s d}) ^ \frac{1}{2} = a ^ {\frac{1}{2} \cdot 2 \cdot 2 ^ {s - 1} d} =
  a ^ {2 ^ {s - 1} d}\\
a_2 = \sqrt{a ^ {2 ^ {s - 1} d}} = (a ^ {2 ^ {s - 1} d}) ^ \frac{1}{2} = a ^ {\frac{1}{2} \cdot 2 \cdot 2 ^ {s - 2} d}
  = a ^ {2 ^ {s - 2} d}\\
\ldots\\
a_{last} = a ^ d
$$

So, if we start with $$a ^ d$$ and square it, we'll get $$a ^ {2d}$$, then $$a ^ {2 ^ 2 d}$$, then $$a ^ {2 ^ 3 d}$$,
and ultimately $$a ^ {2 ^ s d}$$, or $$a ^ {n - 1}$$. What's the advantage of starting from the non-reducible value and
squaring it, rather than the reducible value and taking its square roots? It sometimes allows us to short-circuit the
process. For instance, as we iterate through the squares of $$a ^ d$$, if we find an occurrence of -1, we know that
we'll get 1 when we square it, and 1 when we square that, and keep on getting 1s until we stop iterating. As a
consequence, we know that we won't find any failing conditions, and can exit early by returning `false` (**step 5.3**).
The same goes for **step 4**: if $$a ^ d \equiv \pm 1 \pmod n$$, we know that each of the following squares will equal
1, so we immediately return `false`.

The failing conditions -- ie those that cause the algorithm to return `true` -- might not be immediately clear. In
**5.2**, we know that, if $$x = 1$$, we've violated **assumption 2**, because that implies that the previous value of
$$x$$ was not equivalent to $$\pm 1 \pmod n$$. Wait, why? Because if it were equal to -1, we would've already returned
via **5.3** in the previous iteration, and if it were $$1$$, then we would've returned either from **5.3** in an
earlier iteration still or **4** at the very beginning. We also return `true` when we hit **6**, because we know that
by that point, if **assumption 1** is:

  1. true, and $$x = a ^ {n - 1} \equiv 1 \pmod n$$, then the previous value of $$x$$ can't be either 1 or -1 because
     we would already have returned via either **4** or **5.3**.
  2. false, then by definition $$n$$ can't be prime, since the assumption *must* hold true for prime $$n$$

{% highlight python table %}
def decompose_to_factors_of_2(num):
    s = 0
    d = num

    while d % 2 == 0:
        d //= 2
        s += 1

    return s, d
{% endhighlight %}

# encrypt/decrypt messages
